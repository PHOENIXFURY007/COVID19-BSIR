{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from matplotlib import rc\n",
    "from scipy.interpolate import interp2d, RegularGridInterpolator\n",
    "from scipy.optimize import minimize_scalar, minimize\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1                      # change in time (one day)\n",
    "gamma = 1.0/18              # recovery/death rate for group\n",
    "theta = 0.75                # level of obedience\n",
    "L_max = np.array([0.7,1])   # max amount of lockdown possible\n",
    "M = 2                       # number of groups\n",
    "P = np.array([0.818, 0.182])  # population of each group\n",
    "\n",
    "w = np.array([1,0])              # productivity in normal times\n",
    "ir = 0.00001/365                 # daily interest rate\n",
    "chi = np.ones(2) * 0.2      # non-pecuniary value of life\n",
    "career = np.array([20*365, 0])   # length of remaining career, on avg.\n",
    "\n",
    "ICU_max = 0.0003                    # ICU capacity (based on 30 beds/100,000 people)\n",
    "iota = np.array([0.01683, 0.0481])   # percentage of infected that are sent to ICU\n",
    "\n",
    "nu = 1.5/365              # probability of vaccine/cure arrival (expected arrival 1.5 years)\n",
    "\n",
    "beta_0 = 0.2            # base transmission rate\n",
    "rho_0 = 0.75            # inter-group interaction parameter\n",
    "\n",
    "alpha_I = 1         # if everyone infected, can reduce transmission to e^(-alpha_I)\n",
    "wfh = 0.40           # percent of employees able to work from home\n",
    "alpha_L = 0.00001   # indirect deaths by lockdown level (75000 + 30000)/325 million/100\n",
    "alpha_E = 0.42         # employment loss\n",
    "eta = 10000             # penalty for exceeding ICU capacity\n",
    "F = 1             # constant for future deaths due to missed health screenings, only part of financial calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions for Accelerated Policy Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_wrapper(g, bds, args):\n",
    "\n",
    "    objective = lambda x: g(x, *args)\n",
    "    result = minimize(objective, np.array([0,1]), method='L-BFGS-B', jac = None, bounds=bds)\n",
    "    minimizer, minimum = result.x, result.fun\n",
    "    return minimizer, minimum\n",
    "\n",
    "class OptimalGrowthModel:\n",
    "\n",
    "    def __init__(self,\n",
    "                 u,            # utility function\n",
    "                 f,            # production function\n",
    "                 grid_size\n",
    "                 ):\n",
    "\n",
    "        self.u, self.f = u, f\n",
    "\n",
    "        # Set up grids\n",
    "        self.coarseS = np.linspace(1e-4, 1, grid_size)\n",
    "        self.coarseI = np.linspace(1e-4, 0.5, grid_size)\n",
    "        #self.coarse = np.meshgrid(self.coarseS, self.coarseI)\n",
    "\n",
    "        self.fineS = np.linspace(1e-4, 1, (grid_size*2))\n",
    "        self.fineI = np.linspace(1e-4, 0.5, (grid_size*2))\n",
    "        #self.fine = np.meshgrid(self.fineS, self.fineI)\n",
    "\n",
    "    def state_action_value_VI(self, c, y, v_array):\n",
    "\n",
    "        u, f = self.u, self.f\n",
    "\n",
    "        v = RegularGridInterpolator((self.coarseS, self.coarseS, self.coarseI, self.coarseI), v_array, method = 'linear')\n",
    "\n",
    "        (s0, i0, r0, d0) = f(c, y)\n",
    "\n",
    "        value = u(c,y)*dt + np.exp(-(nu+ir)*dt) * v(np.concatenate((s0, i0)))\n",
    "\n",
    "        return value\n",
    "\n",
    "    def state_action_value_PI(self, c, y, v_array):\n",
    "\n",
    "        u, f = self.u, self.f\n",
    "\n",
    "        v = RegularGridInterpolator((self.fineS, self.fineS, self.fineI, self.fineI), v_array, method = 'linear')\n",
    "        (s0, i0, r0, d0) = f(c, y)\n",
    "        value = u(c,y)*dt + np.exp(-(nu+ir)*dt) * v(np.concatenate((s0, i0)))\n",
    "\n",
    "        return value\n",
    "\n",
    "def VI(v, og):\n",
    "    \n",
    "    v_new = np.empty_like(v)\n",
    "    v_greedy = np.zeros((N, N, N, N, 2))\n",
    "\n",
    "    bds = [(1e-4, L_max[0]), (1e-4, L_max[1])]\n",
    "\n",
    "    sy,so,iy,io = 0,0,0,0\n",
    "\n",
    "    for i in range(len(og.coarseS)):\n",
    "        for j in range(len(og.coarseS)):\n",
    "            for k in range(len(og.coarseI)):\n",
    "                for l in range(len(og.coarseI)):\n",
    "                    bellman = np.array([[og.coarseS[i], og.coarseS[j]],[og.coarseI[k], og.coarseI[l]]])\n",
    "                    b2 = np.sum(bellman, axis = 0)\n",
    "                    if(b2[0] <= (P[0] + 0.02) and b2[1] <= (P[1] + 0.02)):\n",
    "                        if(np.sum(bellman) >= 0.4):\n",
    "                            c_star, v_max = minimize_wrapper(og.state_action_value_VI, bds, (bellman, v))\n",
    "                            v_greedy[i][j][k][l] = c_star\n",
    "                            v_new[i][j][k][l] = v_max\n",
    "                            sy,so,iy,io = i, j, k, l\n",
    "                        else:\n",
    "                            v_greedy[i][j][k][l] = np.zeros(2)\n",
    "                            v_new[i][j][k][l] = 0\n",
    "\n",
    "                    else: # exterior, set to same value as boundary\n",
    "                        v_new[i][j][k][l] = v_new[sy][so][iy][io]\n",
    "                        v_greedy[i][j][k][l] = v_greedy[sy][so][iy][io]\n",
    "\n",
    "    return v_greedy, v_new\n",
    "\n",
    "def policy_eval(policy, og, v0, max_iter, theta=1):\n",
    "    # Initialize the value function\n",
    "    currV = v0\n",
    "    newV = np.empty_like(currV)\n",
    "    iter = 0\n",
    "    delta = theta + 1\n",
    "\n",
    "    sy,so,iy,io = 0,0,0,0\n",
    "\n",
    "    # While our value function is worse than the threshold theta\n",
    "    while (delta > theta) and iter < max_iter:\n",
    "        # Keep track of the update done in value function\n",
    "        delta = 0\n",
    "        # For each state, look ahead one step at each possible action and next state\n",
    "        for i in range(len(og.coarseS)):\n",
    "            for j in range(len(og.coarseS)):\n",
    "                for k in range(len(og.coarseI)):\n",
    "                    for l in range(len(og.coarseI)):\n",
    "                        v = 0\n",
    "                        a = policy[i][j][k][l]\n",
    "                        bellman = np.array([[og.fineS[i], og.fineS[j]],[og.fineI[k], og.fineI[l]]])\n",
    "                        b2 = np.sum(bellman, axis = 0)\n",
    "                        if(b2[0] <= (P[0] + 0.02) and b2[1] <= (P[1] + 0.02)):\n",
    "                            if(np.sum(bellman) >= 0.4):\n",
    "                                v = og.state_action_value_PI(a, bellman, currV)\n",
    "                                delta = max(delta, np.abs(v - currV[i][j][k][l]))\n",
    "                                newV[i][j][k][l] = v\n",
    "                                sy,so,iy,io = i, j, k, l\n",
    "                            else:\n",
    "                                newV[i][j][k][l] = 0\n",
    "\n",
    "                        else: # exterior, set to same value as boundary\n",
    "                            newV[i][j][k][l] = newV[sy][so][iy][io]\n",
    "\n",
    "        # Stop evaluating once our value function update is below a threshold\n",
    "        #print(f\"policy evaluation iteration: {iter}\")\n",
    "        iter += 1\n",
    "        currV = newV\n",
    "\n",
    "    return newV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration to Initialize Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_model_VI(og, v0, max_iter, verbose=True,print_skip=5):\n",
    "    \n",
    "    v = v0\n",
    "    tol_coarse = 4\n",
    "    iter = 0\n",
    "    error = tol_coarse + 1\n",
    "    \n",
    "    while (iter <= max_iter and error > tol_coarse):\n",
    "        v_greedy, v_new = VI(v, og)\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        if (verbose and iter % print_skip == 0):\n",
    "            print(f\"Error at iteration {iter} is {error}.\")\n",
    "        iter += 1\n",
    "        v = v_new\n",
    "    \n",
    "    return v_greedy, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def solve_model_API(og, v0, p0, max_iter, verbose=True,print_skip=5):\n",
    "    \n",
    "    #initialize V and policy for policy iterations\n",
    "    interpV = RegularGridInterpolator((og.coarseS, og.coarseS, og.coarseI, og.coarseI), v0, method = \"linear\")\n",
    "    interpPi = RegularGridInterpolator((og.coarseS, og.coarseS, og.coarseI, og.coarseI), p0, method = \"linear\")\n",
    "\n",
    "    V = np.zeros((len(og.fineS), len(og.fineS), len(og.fineI), len(og.fineI)))\n",
    "    policy = np.zeros((len(og.fineS), len(og.fineS), len(og.fineI), len(og.fineI), 2))\n",
    "\n",
    "    print(\"Initializing value function and policy function\")\n",
    "    for i in range(len(og.fineS)):\n",
    "        for j in range(len(og.fineS)):\n",
    "            for k in range(len(og.fineI)):\n",
    "                for l in range(len(og.fineI)):\n",
    "                    pt = np.array([og.fineS[i], og.fineS[j], og.fineI[k], og.fineI[l]])\n",
    "                    V[i][j][k][l] = interpV(pt)\n",
    "                    policy[i][j][k][l] = interpPi(pt)\n",
    "  \n",
    "    #policy iteration\n",
    "    iter = 0\n",
    "    policy_stable = False\n",
    "    min_time = 0\n",
    "\n",
    "    bds = [(1e-4, L_max[0]), (1e-4, L_max[1])]\n",
    "\n",
    "    while (not policy_stable) and (iter < max_iter):\n",
    "\n",
    "        print(\"Policy eval\")\n",
    "        start_time = time.time()\n",
    "        V = policy_eval(policy, og, V, 10)\n",
    "        print(time.time() - start_time)\n",
    "        policy_stable = True\n",
    "\n",
    "        print(\"Policy improvement\")\n",
    "        start_time = time.time()\n",
    "        for i in range(len(og.fineS)):\n",
    "            for j in range(len(og.fineS)):\n",
    "                for k in range(len(og.fineI)):\n",
    "                    for l in range(len(og.fineI)):\n",
    "                        chosen_a = policy[i][j][k][l]\n",
    "                        bellman = np.array([[og.fineS[i], og.fineS[j]],[og.fineI[k], og.fineI[l]]])\n",
    "                        b2 = np.sum(bellman, axis = 0)\n",
    "                        if(b2[0] <= (P[0] + 0.02) and b2[1] <= (P[1] + 0.02)):\n",
    "                            start_time2 = time.time()\n",
    "                            best_a, min_val = maximize(og.state_action_value_PI, bds, (bellman, V))\n",
    "                            min_time += (time.time() - start_time2)\n",
    "                            if np.any(np.abs(chosen_a - best_a) >= 0.001):\n",
    "                                policy_stable = False\n",
    "                            policy[i][j][k][l] = best_a\n",
    "        print(time.time() - start_time)\n",
    "\n",
    "        if verbose and iter % print_skip == 0:\n",
    "            print(f\"Policy improvement iteration {iter}\")\n",
    "        iter += 1\n",
    "\n",
    "    if iter == max_iter:\n",
    "        print(\"Policy improvement failed to converge!\")\n",
    "\n",
    "    print(f\"Time for minimization: {min_time}\")\n",
    "\n",
    "    return policy, V, v_greedy, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Related to SIR Model, Cost and Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indir(L_curr):\n",
    "    return alpha_L * L_curr\n",
    "\n",
    "def empl(L_e):\n",
    "    return alpha_E * w * L_e\n",
    "\n",
    "def ICU(I_i):\n",
    "    ICU_curr = np.sum(I_i * iota)\n",
    "    return np.maximum(0, (ICU_curr - ICU_max) * eta)\n",
    "\n",
    "def phi(I_p):\n",
    "    I_total = np.sum(I_p)\n",
    "    #a = np.array([0.000634*gamma, 0.00845*gamma])\n",
    "    #b = np.array([0.00845*gamma, 0.1127*gamma])\n",
    "    a = np.array([0.01*gamma, 0.06*gamma])\n",
    "    b = np.array([0.06*gamma, 0.1*gamma])\n",
    "    return a + b * I_total\n",
    "\n",
    "def betaBSIR(I_b):\n",
    "    beta = beta_0*np.exp(-alpha_I*np.sum(I_b))\n",
    "    rho = np.array([[1,rho_0],[rho_0,1]])\n",
    "    return beta*rho\n",
    "\n",
    "def dynamics(L_d, state):\n",
    "# population levels are in absolute terms (i.e. S_y = 0.2 => 20% of ENTIRE pop)\n",
    "#state = [[S_y, S_o],[I_y, I_o]] 2x2 matrix\n",
    "\n",
    "    R_d = P - np.sum(state, axis = 0) #2D array\n",
    "    S_d = state[0] #length 2 array\n",
    "    I_d = state[1] #length 2 array\n",
    "\n",
    "#    beta = betaBSIR(I_d)\n",
    "    if(np.sum(R_d) < 0.6):\n",
    "        beta = betaBSIR(I_d)\n",
    "    else:\n",
    "        beta = np.zeros((M,M))     #2x2 matrix\n",
    "\n",
    "    deathRate = phi(I_d)    #length 2 array\n",
    "    indirDeath = indir(L_d) #scalar\n",
    "    recoveryRate = gamma*np.ones(M) - deathRate #length 2 array\n",
    "\n",
    "    sum_I = np.dot(beta,((1 - theta*L_d)*I_d)) #length 2 array\n",
    "\n",
    "    # should all be length 2 arrays\n",
    "    dI = sum_I * (1 - theta * L_d) * S_d - gamma * I_d\n",
    "    dS = -dI - gamma * I_d - indirDeath * S_d\n",
    "    dR = recoveryRate * I_d - indirDeath * R_d\n",
    "    dD = deathRate * I_d + indirDeath * (S_d + R_d)\n",
    "\n",
    "    D_new = dt*dD\n",
    "    D_new = np.minimum(D_new, 1.0)\n",
    "    D_new = np.maximum(D_new, 0)\n",
    "\n",
    "    S_new = (S_d + dt*dS)/(1 - D_new)\n",
    "    S_new = np.minimum(S_new, 1.0)\n",
    "    S_new = np.maximum(S_new, 1e-4)\n",
    "\n",
    "    I_new = (I_d + dt*dI)/(1 - D_new)\n",
    "    I_new = np.minimum(I_new, 0.5)\n",
    "    I_new = np.maximum(I_new, 1e-4)\n",
    "\n",
    "    R_new = (R_d + dt*dR)/(1 - D_new)\n",
    "    R_new = np.minimum(R_new, 1.0)\n",
    "    R_new = np.maximum(R_new, 1e-4)\n",
    "\n",
    "    return (S_new, I_new, R_new, D_new)\n",
    "\n",
    "def cost(L_c, state):\n",
    "    #state = [[S_y, S_o],[I_y, I_o]] 2x2 matrix\n",
    "\n",
    "    S_c = state[0]  #length 2 array\n",
    "    I_c = state[1] #length 2 array\n",
    "    R_c = P - (S_c + I_c) #length 2 arrays\n",
    "\n",
    "    if(np.sum(I_c) == 0):\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = np.sum(w*L_c*(1 - wfh)*(S_c + I_c + R_c) #lost salary\n",
    "        + ((chi + w)/ir * (1 - np.exp(-ir*career))) * phi(I_c) * I_c #COVID deaths\n",
    "        + (w/ir * (1 - np.exp(-ir*career))) * indir(L_c) * (F + S_c + R_c) #non-COVID deaths\n",
    "        + empl(L_c)*(S_c + R_c))  #future unemployment costs\n",
    "        + ICU(I_c) #cost of exceeding ICU capacity\n",
    "\n",
    "    return cost #scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "N = 11\n",
    "\n",
    "og = OptimalGrowthModel(u=cost, f=dynamics, grid_size = N)\n",
    "v = np.zeros((N, N, N, N)) # An initial condition, just set value function to 0\n",
    "\n",
    "print(\"Value Iterations\")\n",
    "p_VI, v_VI = solve_model_VI(og, v, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy Iterations\")\n",
    "p_PI, v_PI = solve_model_API(og, v_VI, p_VI, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_N = 300\n",
    "\n",
    "D = np.zeros((T_N,M))                   #dead, [j][t]\n",
    "D_cov = np.zeros((T_N,M))\n",
    "S = np.zeros((T_N,M))                   #susceptible, [j][t]\n",
    "I = np.zeros((T_N,M))                   #infected, [j][t]\n",
    "R = np.zeros((T_N,M))                   #recovered, [j][t]\n",
    "L_opt = np.zeros((T_N,M))\n",
    "\n",
    "S_base = np.zeros((T_N,M))\n",
    "I_base = np.zeros((T_N,M))\n",
    "R_base = np.zeros((T_N,M))\n",
    "D_base = np.zeros((T_N,M))\n",
    "\n",
    "S_0 = 0.98\n",
    "I_0 = 0.01\n",
    "R_0 = 0.01\n",
    "D_0 = 0.0\n",
    "\n",
    "#initialize arrays\n",
    "S[0] = S_0*np.ones(2)\n",
    "I[0] = I_0*np.ones(2)\n",
    "R[0] = R_0*np.ones(2)\n",
    "D[0] = D_0*np.ones(2)\n",
    "D_cov[0] = D_0*np.ones(2)\n",
    "\n",
    "S_base[0] = S_0*np.ones(2)\n",
    "I_base[0] = I_0*np.ones(2)\n",
    "R_base[0] = R_0*np.ones(2)\n",
    "D_base[0] = D_0*np.ones(2)\n",
    "\n",
    "\n",
    "interpControl = RegularGridInterpolator((gridS, gridS, gridI, gridI), p_PI, method = 'linear', bounds_error = False)\n",
    "\n",
    "herd = -1\n",
    "herd_base = -1\n",
    "gdp = 0\n",
    "gdp_base = 0\n",
    "l_base = np.zeros(2)\n",
    "\n",
    "for t in range(T_N-1):\n",
    "    s_curr = S[t]*P\n",
    "    i_curr = I[t]*P\n",
    "    r_curr = R[t]*P\n",
    "    d_curr = D[t]*P\n",
    "\n",
    "    if (herd < 0 and np.sum(r_curr) >= 0.6):\n",
    "        herd = t\n",
    "\n",
    "    try:\n",
    "        #L_opt[t] = interpControl(np.concatenate((s_curr, i_curr)))\n",
    "        if (herd > 0):\n",
    "            L_opt[t] = 0\n",
    "        else:\n",
    "            L_opt[t] = interpControl(np.concatenate((s_curr, i_curr)))\n",
    "    except ValueError :\n",
    "        print(s_curr)\n",
    "        print(i_curr)\n",
    "        print(f\"ValueError at line 425, t = {t}\")\n",
    "\n",
    "    l_curr = L_opt[t]\n",
    "\n",
    "    gdp += cost(l_curr, [s_curr, i_curr])*dt\n",
    "\n",
    "    (S_new, I_new, R_new, D_new) = dynamics(l_curr, [s_curr, i_curr])\n",
    "    P = (P - D_new)/np.sum(P - D_new)\n",
    "    I[t+1] = I_new/P\n",
    "    S[t+1] = S_new/P\n",
    "    D[t+1] = D_new/P + D[t]\n",
    "    R[t+1] = R_new/P\n",
    "    D_cov[t+1] = D_cov[t] + dt*phi(i_curr)*i_curr/P\n",
    "\n",
    "    #######################\n",
    "    s_b = S_base[t]*P\n",
    "    i_b = I_base[t]*P\n",
    "    r_b = R_base[t]*P\n",
    "    d_b = D_base[t]*P\n",
    "\n",
    "    if (herd_base < 0 and np.sum(r_b) >= 0.6):\n",
    "        herd_base = t\n",
    "\n",
    "    gdp_base += cost(l_base, [s_b, i_b])\n",
    "    (S_2, I_2, R_2, D_2) = dynamics(l_base, [s_b, i_b])\n",
    "    P = (P - D_2)/np.sum(P - D_2)\n",
    "    S_base[t+1] = S_2/P\n",
    "    I_base[t+1] = I_2/P\n",
    "    R_base[t+1] = R_2/P\n",
    "    D_base[t+1] = D_2/P + D_base[t]\n",
    "\n",
    "#manually calculate last step's optimal policy\n",
    "s_curr = S[T_N -1]*P\n",
    "i_curr = I[T_N -1]*P\n",
    "r_curr = R[T_N -1]*P\n",
    "d_curr = D[T_N -1]*P\n",
    "\n",
    "if (herd < 0 and np.sum(r_curr) >= 0.6):\n",
    "    herd = T_N -1 #marks the arrival of herd immunity\n",
    "\n",
    "try:\n",
    "    if (herd > 0): \n",
    "        L_opt[T_N -1] = 0 #already reached herd immunity (helps prevent numerical diffusion)\n",
    "    else:\n",
    "        L_opt[T_N -1] = interpControl(np.concatenate((s_curr, i_curr))) #get optimal control\n",
    "except ValueError :\n",
    "    print(s_curr)\n",
    "    print(i_curr)\n",
    "    print(f\"ValueError at control interpolation, t = {T_N -1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = gdp * (133.29*220958853)/(21.43*(10**10))\n",
    "gdp_base = gdp_base * (133.29*220958853)/(21.43*(10**10))\n",
    "deaths = D[T_N - 1]*P\n",
    "deaths_base = D_base[T_N - 1]*P\n",
    "deaths_cov = D_cov[T_N - 1]*P\n",
    "\n",
    "D_tot = np.sum(D*P, axis = 1)\n",
    "I_tot = np.sum(I*P, axis = 1)\n",
    "DB_tot = np.sum(D_base*P, axis = 1)\n",
    "IB_tot = np.sum(I_base*P, axis = 1)\n",
    "\n",
    "D = np.transpose(D)                   #dead, [j][t]\n",
    "S = np.transpose(S)                #susceptible, [j][t]\n",
    "I = np.transpose(I)                   #infected, [j][t]\n",
    "R = np.transpose(R)                   #recovered, [j][t]\n",
    "L_opt = np.transpose(L_opt)\n",
    "D_cov = np.transpose(D_cov)\n",
    "S_base = np.transpose(S_base)\n",
    "I_base = np.transpose(I_base)\n",
    "R_base = np.transpose(R_base)\n",
    "D_base = np.transpose(D_base)\n",
    "\n",
    "y_lockdown = L_opt[0][L_opt[0] > 1e-4]\n",
    "avg_y = np.sum(y_lockdown)/len(y_lockdown)\n",
    "len_y = len(y_lockdown)\n",
    "o_lockdown = L_opt[1][L_opt[1] > 1e-4]\n",
    "avg_o = np.sum(o_lockdown)/len(o_lockdown)\n",
    "len_o = len(o_lockdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('text', usetex=True)\n",
    "#plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "x = np.linspace(0, T_N*dt, T_N)\n",
    "\n",
    "txt = f'''$S_0 = {S_0}$, $I_0 = {I_0}$, $R_0 = {R_0}$, $D_0 = {D_0}$, $\\\\rho = {rho_0}$, $\\\\chi = {chi}$, $r = {ir*365*100}\\\\%$, $\\\\nu  = {round(nu*365, 2)}$, $\\\\alpha_L = {alpha_L}$, $\\\\alpha_I = {alpha_I}$, $\\\\alpha_E = {alpha_E}$, $\\\\eta = {eta}$, F = {F}, wfh = {wfh}\n",
    "Avg. Lockdown (20-64): {round(avg_y, 4)} over {len_y} days, Avg. Lockdown (65+): {round(avg_o, 4)} over {len_o} days, GDP Loss: {round(gdp, 4)}\\\\% vs {round(gdp_base, 4)}\\\\%, Total Deaths: {round(np.sum(deaths)*100, 4)}\\\\% vs {round(np.sum(deaths_base)*100, 4)}\\\\%'''\n",
    "\n",
    "print(txt)\n",
    "print(f\"{round(avg_y, 4)} & {round(avg_o, 4)} & {len_o} days & {round(gdp,4)}\\\\% & {round(np.sum(deaths)*100, 4)}\\\\%\")\n",
    "print(f\"{round(np.sum(deaths)*100, 4)}\\\\% & {round(np.sum(deaths_cov)*100, 4)}\\\\%{round(deaths[0]*100, 4)}\\\\% &  {round(deaths_cov[0]*100, 4)}\\\\% & {round(deaths[1]*100, 4)}\\\\% & {round(deaths_cov[1]*100, 4)}\\\\% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs1 = plt.subplots()\n",
    "\n",
    "L_y = axs1.plot(x,L_opt[0],marker ='', ls = 'solid', color = 'darkblue', label = 'Lockdown (20-64)')\n",
    "L_o = axs1.plot(x,L_opt[1],marker ='', ls = 'dashed', color = 'darkblue', label = 'Lockdown (65+)')\n",
    "if(herd>0):\n",
    "    axs1.vlines(x=herd, ymin=0, ymax=1, color='c', linestyle='-', label = 'Herd Immunity')#, Lockdown')\n",
    "#if(herd_base>0):\n",
    "#    axs1.vlines(x=herd_base, ymin=0, ymax=1, color='maroon', linestyle=':', label = 'Herd Immunity, No Lockdown')\n",
    "axs1.set_xlabel('Time')\n",
    "axs1.set_ylabel('Lockdown Rate')\n",
    "axs1.set_title('Lockdown Policy')\n",
    "axs1.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs2 = plt.subplots()\n",
    "\n",
    "#I_t = axs2.plot(x,I_tot,marker ='', ls = 'solid', color = 'blue', label = 'Infected, Lockdown')\n",
    "#J_t = axs2.plot(x,IB_tot,marker ='', ls = 'dashdot', color = 'red', label = 'Infected, No Lockdown')\n",
    "I_y = axs2.plot(x,I[0],marker ='', ls = 'solid', color = 'red', label = 'Infected (20-64)')#', Lockdown')\n",
    "#J_y = axs2.plot(x,I_base[0],marker ='', ls = 'dashdot', color = 'red', label = 'Infected (30-64), No Lockdown')\n",
    "S_y = axs2.plot(x,S[0],marker ='',  ls = 'solid', color = 'blue', label = 'Susceptible (20-64)')\n",
    "R_y = axs2.plot(x,R[0],marker ='', ls = 'solid', color = 'green', label = 'Recovered (20-64)')\n",
    "D_y = axs2.plot(x,D[0],marker ='', ls = 'solid', color = 'black', label = 'Dead (20-64)')\n",
    "\n",
    "I_o = axs2.plot(x,I[1],marker ='', ls = 'dashed', color = 'red', label = 'Infected (65+)')#', Lockdown')\n",
    "#J_o = axs2.plot(x,I_base[1],marker ='', ls = 'dotted', color = 'red', label = 'Infected (65+), No Lockdown')\n",
    "S_o = axs2.plot(x,S[1],marker ='',  ls = 'dashed', color = 'blue', label = 'Susceptible (65+)')\n",
    "R_o = axs2.plot(x,R[1],marker ='', ls = 'dashed', color = 'green', label = 'Recovered (65+)')\n",
    "D_o = axs2.plot(x,D[1],marker ='', ls = 'dashed', color = 'black', label = 'Dead (65+)')\n",
    "\n",
    "if(herd>0):\n",
    "    axs2.vlines(x=herd, ymin=0, ymax=1, color='c', linestyle='-', label = 'Herd Immunity')#, Lockdown')\n",
    "#if(herd_base>0):\n",
    "#    axs2.vlines(x=herd_base, ymin=0, ymax=0.4, color='maroon', linestyle=':', label = 'Herd Immunity, No Lockdown')\n",
    "axs2.set_xlabel('Time')\n",
    "axs2.set_ylabel('Proportion of Group')\n",
    "axs2.set_title('Population Dynamics')\n",
    "axs2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axs3 = plt.subplots()\n",
    "\n",
    "#D_t = axs3.plot(x,D_tot,marker ='', ls = 'solid', color = 'black', label = 'Deaths, Lockdown')\n",
    "#E_t = axs3.plot(x,DB_tot,marker ='', ls = 'dashdot', color = 'red', label = 'Deaths, No Lockdown')\n",
    "D_y = axs3.plot(x,D[0],marker ='', ls = 'solid', color = 'black', label = 'Dead (20-64)')#', Lockdown')\n",
    "D_o = axs3.plot(x,D[1],marker ='', ls = 'dashed', color = 'black', label = 'Dead (65+)')#', Lockdown')\n",
    "#E_y = axs3.plot(x,D_base[0],marker ='', ls = 'dashdot', color = 'red', label = 'Dead (20-64), No Lockdown')\n",
    "#E_o = axs3.plot(x,D_base[1],marker ='', ls = 'dotted', color = 'red', label = 'Dead (65+), No Lockdown')\n",
    "#C_y = axs3.plot(x,D_cov[0],marker ='', ls = 'solid', color = 'grey', label = 'COVID (20-64)')\n",
    "#C_o = axs3.plot(x,D_cov[1],marker ='', ls = 'dashed', color = 'grey', label = 'COVID (65+)')\n",
    "\n",
    "if(herd>0):\n",
    "    axs3.vlines(x=herd, ymin=0, ymax=np.max(D[1]), color='c', linestyle='-', label = 'Herd Immunity, Lockdown')\n",
    "#if(herd_base > 0):\n",
    "#    axs3.vlines(x=herd_base, ymin=0, ymax=np.max(DB_tot), color='maroon', linestyle=':', label = 'Herd Immunity, No Lockdown')\n",
    "\n",
    "axs3.set_xlabel('Time')\n",
    "axs3.set_ylabel('Proportion of Group')\n",
    "axs3.set_title('Deaths')\n",
    "axs3.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
